"""
==============================================================================
 í•œêµ­ ìƒì¥ ETF Managed Portfolio ìœ ë‹ˆë²„ìŠ¤ ë¹Œë” v5.2
==============================================================================
 [ìƒˆë¡œìš´ ì›Œí¬í”Œë¡œìš° â€” ê°€ë²¼ìš´ í•„í„° ë¨¼ì €, ë¬´ê±°ìš´ ìˆ˜ì§‘ì€ ë‚˜ì¤‘ì—]
  Step 1: ì „ì²´ ETF í‹°ì»¤ + ì´ë¦„ ìˆ˜ì§‘ (ê°€ë²¼ì›€)
  Step 2: ìœ í˜• í•„í„°ë§ â€” í‚¤ì›Œë“œ ê¸°ë°˜ (ê°€ë²¼ì›€)
  Step 3: ì‹œê°€ì´ì•¡ ë°ì´í„° ìˆ˜ì§‘ â†’ 100ì–µ ë¯¸ë§Œ ì œì™¸ (ì¤‘ê°„)
  Step 4: ìµœì¢… ë¦¬ìŠ¤íŠ¸ì— ëŒ€í•´ ê°€ê²©/ìƒì¥ì¼/PDF ìˆ˜ì§‘ (ë¬´ê±°ì›€)
  Step 5: ìˆ˜ìµë¥ /BM/ìˆœìœ„ ê³„ì‚° + ì—‘ì…€ ì €ì¥

 pip install pykrx pandas openpyxl tqdm
==============================================================================
"""

import pandas as pd
import numpy as np
from pykrx import stock
from datetime import datetime, timedelta
from tqdm import tqdm
import time, warnings, os, re, pickle, json
from concurrent.futures import ThreadPoolExecutor, as_completed
from urllib.request import urlopen, Request
from urllib.error import URLError

warnings.filterwarnings("ignore")


# ============================================================================
# ì„¤ì •
# ============================================================================
class Config:
    BASE_DATE = None
    MIN_MARKET_CAP_BILLIONS = 200      # í´ë¼ìš°ë“œ ë°°í¬ ì‹œ ë©”ëª¨ë¦¬ ì ˆì•½
    PRICE_HISTORY_DAYS = 365
    API_DELAY = 0.05
    MAX_WORKERS = 8                    # í´ë¼ìš°ë“œ ì•ˆì •ì„±
    USE_CACHE = True
    CACHE_DIR = "./etf_cache"
    OUTPUT_DIR = "./etf_universe_output"
    TOP_N_HOLDINGS = 10

    EXCLUDE_KEYWORDS = [
        'ì±„ê¶Œ', 'êµ­ê³ ì±„', 'í†µì•ˆì±„', 'êµ­ì±„', 'íšŒì‚¬ì±„', 'í•˜ì´ì¼ë“œ', 'í¬ë ˆë”§',
        'ì¤‘ê¸°ì±„', 'ì¥ê¸°ì±„', 'ë‹¨ê¸°ì±„', 'ì´ˆì¥ê¸°', 'ê¸ˆë¦¬', 'ì„ ì§„êµ­ì±„',
        'KISêµ­ì±„', 'êµ­ê³µì±„', 'ìš°ëŸ‰ì±„',
        'íŠ¹ìˆ˜ì±„', 'ì „ë‹¨ì±„', 'ë¬¼ê°€ì±„', 'ê³µì‚¬ì±„', 'ì€í–‰ì±„', 'ì¢…ê¸ˆì±„',
        'ì¹´ë“œì±„', 'ìºí”¼íƒˆì±„', 'ì§€ë°©ì±„', 'ë„ì‹œì±„',
        'ë¨¸ë‹ˆë§ˆì¼“', 'CDê¸ˆë¦¬', 'KOFR', 'ë‹¨ê¸°ìê¸ˆ', 'ì˜ˆê¸ˆ',
        'íŒŒí‚¹', 'ë‹¨ê¸°ì±„ê¶Œ', 'KCD',
        'TDF', 'TRF',
        'í˜¼í•©', 'ìì‚°ë°°ë¶„',
        'ì»¤ë²„ë“œì½œ', 'COVERED CALL', 'COVERED',
        'ë ˆë²„ë¦¬ì§€', 'ì¸ë²„ìŠ¤', '2X', 'ê³±ë²„ìŠ¤',
    ]


# ============================================================================
# ìœ í‹¸ë¦¬í‹°
# ============================================================================
def find_latest_business_date(max_lookback=30):
    """ìµœê·¼ ì˜ì—…ì¼ ì°¾ê¸°
    - KST(í•œêµ­ì‹œê°„) ê¸°ì¤€ìœ¼ë¡œ ê³„ì‚° (Streamlit CloudëŠ” UTC)
    - ì£¼ë§ ìë™ ê±´ë„ˆë›°ê¸°
    - ì¥ ë§ˆê° ì „ì´ë©´ ì „ ì˜ì—…ì¼ ì‚¬ìš©
    - ê³µíœ´ì¼ ëŒ€ë¹„ ìµœëŒ€ 30ì¼ ë’¤ë¡œ
    """
    # UTC â†’ KST (UTC+9)
    try:
        from zoneinfo import ZoneInfo
        now_kst = datetime.now(ZoneInfo("Asia/Seoul"))
    except Exception:
        # Python 3.8 ì´í•˜ ë˜ëŠ” zoneinfo ì—†ëŠ” í™˜ê²½
        now_kst = datetime.utcnow() + timedelta(hours=9)

    today_kst = now_kst.date()
    hour_kst = now_kst.hour

    print(f"  ğŸ• í˜„ì¬ KST: {now_kst.strftime('%Y-%m-%d %H:%M')}")

    # ì¥ ë§ˆê°(15:30) ì „ì´ë©´ ì˜¤ëŠ˜ ë°ì´í„° ì—†ì„ ìˆ˜ ìˆìŒ â†’ ì „ì¼ë¶€í„° íƒìƒ‰
    start_offset = 0 if hour_kst >= 18 else 1  # 18ì‹œ ì´í›„ë©´ ë‹¹ì¼ ë°ì´í„° í™•ë³´

    for i in range(start_offset, max_lookback):
        d = today_kst - timedelta(days=i)

        # ì£¼ë§ ê±´ë„ˆë›°ê¸° (í† =5, ì¼=6)
        if d.weekday() >= 5:
            continue

        ds = d.strftime("%Y%m%d")
        try:
            tickers = stock.get_etf_ticker_list(ds)
            if tickers is not None and len(tickers) > 0:
                print(f"  âœ… ìµœê·¼ ì˜ì—…ì¼: {ds}")
                return ds
        except Exception as e:
            print(f"  âš ï¸ {ds} ì¡°íšŒ ì‹¤íŒ¨: {e}")
            time.sleep(0.5)  # API ë¶€í•˜ ë°©ì§€
            continue

    # ìµœí›„ ìˆ˜ë‹¨: ì£¼ë§ ë¬´ì‹œí•˜ê³  ë‹¨ìˆœ ë’¤ë¡œ
    fallback = today_kst - timedelta(days=3)
    while fallback.weekday() >= 5:
        fallback -= timedelta(days=1)
    ds = fallback.strftime("%Y%m%d")
    print(f"  âš ï¸ fallback ì˜ì—…ì¼: {ds}")
    return ds


def _timer(label):
    """Step íƒ€ì´ë¨¸ (ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €)"""
    class Timer:
        def __enter__(self):
            self.t0 = time.time()
            return self
        def __exit__(self, *a):
            elapsed = time.time() - self.t0
            print(f"  â±ï¸ {label}: {elapsed:.1f}ì´ˆ")
    return Timer()


def _load_cache(name):
    path = os.path.join(Config.CACHE_DIR, name)
    if Config.USE_CACHE and os.path.exists(path):
        try:
            with open(path, 'rb') as f: return pickle.load(f)
        except Exception: pass
    return None

def _save_cache(name, data):
    if Config.USE_CACHE:
        os.makedirs(Config.CACHE_DIR, exist_ok=True)
        with open(os.path.join(Config.CACHE_DIR, name), 'wb') as f:
            pickle.dump(data, f)


# ============================================================================
# Step 1: ì „ì²´ ETF í‹°ì»¤ + ì´ë¦„ (ê°€ë²¼ì›€)
# ============================================================================
def step1_get_tickers_and_names(base_date):
    print("\n" + "="*60)
    print(" Step 1: ì „ì²´ ETF í‹°ì»¤ + ì´ë¦„ ìˆ˜ì§‘")
    print("="*60)

    with _timer("Step 1"):
        tickers = stock.get_etf_ticker_list(base_date)
        print(f"  â†’ ì „ì²´ ETF: {len(tickers)}ê°œ")

        # ìºì‹œ í™•ì¸
        cache_name = f"names_{base_date}.pkl"
        cached = _load_cache(cache_name)
        if cached and len(cached) >= len(tickers) * 0.9:
            print(f"  â†’ ğŸ’¾ ì´ë¦„ ìºì‹œ ë¡œë“œ: {len(cached)}ê°œ")
            etf_names = cached
        else:
            # ë©€í‹°ìŠ¤ë ˆë“œë¡œ ì´ë¦„ ìˆ˜ì§‘
            etf_names = {}
            def fetch_name(t):
                try: return t, stock.get_etf_ticker_name(t)
                except Exception: return t, "N/A"

            with ThreadPoolExecutor(max_workers=Config.MAX_WORKERS) as exe:
                futs = {exe.submit(fetch_name, t): t for t in tickers}
                with tqdm(total=len(tickers), desc="  ì´ë¦„ ì¡°íšŒ") as pbar:
                    for f in as_completed(futs):
                        t, name = f.result()
                        etf_names[t] = name
                        pbar.update(1)

            _save_cache(cache_name, etf_names)

        df = pd.DataFrame({'í‹°ì»¤': tickers,
                            'ETFëª…': [etf_names.get(t, 'N/A') for t in tickers]})
        df = df.set_index('í‹°ì»¤')
        print(f"  âœ… {len(df)}ê°œ ETF ì´ë¦„ ìˆ˜ì§‘ ì™„ë£Œ")
    return df


# ============================================================================
# Step 2: ìœ í˜• í•„í„°ë§ â€” í‚¤ì›Œë“œ ê¸°ë°˜ (ê°€ë²¼ì›€) + ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜
# ============================================================================
def step2_type_filter_and_classify(df):
    print("\n" + "="*60)
    print(" Step 2: ìœ í˜• í•„í„°ë§ + ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜")
    print("="*60)

    t0 = time.time()
    before = len(df)

    def should_exclude(name):
        if pd.isna(name): return True
        s = str(name)
        for kw in Config.EXCLUDE_KEYWORDS:
            if kw.upper() in s.upper(): return True
        if re.search(r'[ê°€-í£]{1,3}ì±„(?![ê¶Œ])', s): return True
        if re.search(r'TRF\d{4}', s.upper()): return True
        return False

    mask = df['ETFëª…'].apply(should_exclude)
    excluded = df[mask]
    df = df[~mask].copy()

    print(f"  â†’ ì œì™¸: {len(excluded)}ê°œ (ì±„ê¶Œ/ë¨¸ë‹ˆë§ˆì¼“/ì»¤ë²„ë“œì½œ/ë ˆë²„ë¦¬ì§€/ì¸ë²„ìŠ¤/TDF ë“±)")
    if len(excluded) > 0:
        for idx, row in excluded.head(10).iterrows():
            print(f"    - {idx} {row['ETFëª…']}")
        if len(excluded) > 10:
            print(f"    ... ì™¸ {len(excluded)-10}ê°œ")

    # ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜
    df = _classify(df)

    print(f"\n  â†’ {before}ê°œ â†’ {len(df)}ê°œ")
    print(f"  â±ï¸ Step 2: {time.time()-t0:.1f}ì´ˆ")
    return df


# ============================================================================
# Step 3: ì‹œê°€ì´ì•¡ ìˆ˜ì§‘ â†’ 100ì–µ ë¯¸ë§Œ ì œì™¸
# ============================================================================
def step3_market_cap_filter(df, base_date, min_cap=100):
    print("\n" + "="*60)
    print(f" Step 3: ì‹œê°€ì´ì•¡ ìˆ˜ì§‘ + {min_cap}ì–µ ì´ìƒ í•„í„°")
    print("="*60)

    t0 = time.time()
    before = len(df)

    # ìºì‹œ í™•ì¸ (v2 â€” ì´ì „ ìºì‹œ ë¬´ì‹œ)
    cache_name = f"mktcap_v2_{base_date}.pkl"
    cached = _load_cache(cache_name)
    if cached is not None and 'ì‹œê°€ì´ì•¡(ì–µì›)' in cached.columns:
        print(f"  â†’ ğŸ’¾ ì‹œì´ ìºì‹œ ë¡œë“œ: {len(cached)}ê°œ")
        df = df.join(cached[['ì‹œê°€ì´ì•¡(ì–µì›)', 'NAV(ì–µì›)']].dropna(how='all'), how='left')
        df = df[df['ì‹œê°€ì´ì•¡(ì–µì›)'].notna() & (df['ì‹œê°€ì´ì•¡(ì–µì›)'] >= min_cap)].copy()
        df['ì‹œê°€ì´ì•¡(ì–µì›)'] = df['ì‹œê°€ì´ì•¡(ì–µì›)'].astype(int)
        print(f"  â†’ {before}ê°œ â†’ {len(df)}ê°œ")
        print(f"  â±ï¸ Step 3: {time.time()-t0:.1f}ì´ˆ")
        return df

    # â”€â”€ ì‹œê°€ì´ì•¡ ìˆ˜ì§‘ â”€â”€
    cap_series = pd.Series(dtype=float, name='ì‹œê°€ì´ì•¡')
    nav_series = pd.Series(dtype=float, name='NAV')

    # ë°©ë²• 1: get_etf_price_by_ticker (ETF ì „ìš©)
    print("  â†’ [1ì°¨] get_etf_price_by_ticker...")
    try:
        df_etf = stock.get_etf_price_by_ticker(base_date)
        print(f"    ì»¬ëŸ¼: {df_etf.columns.tolist()}")
        print(f"    í–‰ ìˆ˜: {len(df_etf)}, ìƒ˜í”Œ ì¸ë±ìŠ¤: {df_etf.index[:3].tolist()}")

        # ì‹œê°€ì´ì•¡ ì»¬ëŸ¼ ì°¾ê¸° (ë‹¤ì–‘í•œ ì´ë¦„)
        for c in df_etf.columns:
            cl = str(c).replace(' ','')
            if 'ì‹œê°€ì´ì•¡' in cl or 'ì‹œì´' in cl:
                cap_series = df_etf[c]; print(f"    âœ… ì‹œì´ ì»¬ëŸ¼: '{c}'"); break
            if 'ìˆœìì‚°' in cl or 'NAV' in cl.upper():
                nav_series = df_etf[c]; print(f"    âœ… NAV ì»¬ëŸ¼: '{c}'")

        # ì¢…ê°€/ê±°ë˜ëŸ‰ë„ ì—¬ê¸°ì„œ ê°€ì ¸ì˜¬ ìˆ˜ ìˆìŒ
        for c in df_etf.columns:
            if 'ì¢…ê°€' in str(c) and 'ì¢…ê°€' not in df.columns:
                df = df.join(df_etf[[c]], how='left')
            if 'ê±°ë˜ëŸ‰' in str(c) and 'ê±°ë˜ëŸ‰' not in df.columns:
                df = df.join(df_etf[[c]], how='left')
    except Exception as e:
        print(f"    âš ï¸ ì‹¤íŒ¨: {e}")

    # ë°©ë²• 2: get_market_cap_by_ticker (ì£¼ì‹ ì‹œì´ â€” ETFë„ í¬í•¨ë  ìˆ˜ ìˆìŒ)
    if cap_series.empty or cap_series.isna().all():
        print("  â†’ [2ì°¨] get_market_cap_by_ticker...")
        try:
            df_mc = stock.get_market_cap_by_ticker(base_date)
            print(f"    ì»¬ëŸ¼: {df_mc.columns.tolist()}, í–‰: {len(df_mc)}")
            # ETF ì¸ë±ìŠ¤ì™€ ê²¹ì¹˜ëŠ”ì§€ í™•ì¸
            overlap = set(df.index) & set(df_mc.index)
            print(f"    ETFì™€ ê²¹ì¹˜ëŠ” í‹°ì»¤: {len(overlap)}ê°œ")
            if len(overlap) > 0 and 'ì‹œê°€ì´ì•¡' in df_mc.columns:
                cap_series = df_mc['ì‹œê°€ì´ì•¡']
                print(f"    âœ… ì‹œì´ í™•ë³´: {cap_series.notna().sum()}ê°œ")
        except Exception as e:
            print(f"    âš ï¸ ì‹¤íŒ¨: {e}")

    # ë°©ë²• 3: ê°œë³„ ETF (ëŠë¦¬ì§€ë§Œ í™•ì‹¤)
    if cap_series.empty or cap_series.isna().all():
        print("  â†’ [3ì°¨] ê°œë³„ ETF ì‹œì´ ìˆ˜ì§‘...")
        cap_data = {}
        def fetch_cap(ticker):
            try:
                r = stock.get_market_cap_by_date(base_date, base_date, ticker)
                if not r.empty and 'ì‹œê°€ì´ì•¡' in r.columns:
                    return ticker, r['ì‹œê°€ì´ì•¡'].iloc[-1]
            except Exception: pass
            return ticker, np.nan

        with ThreadPoolExecutor(max_workers=Config.MAX_WORKERS) as exe:
            futs = {exe.submit(fetch_cap, t): t for t in df.index}
            with tqdm(total=len(df), desc="  ì‹œì´ ìˆ˜ì§‘") as pbar:
                for f in as_completed(futs):
                    t, v = f.result()
                    cap_data[t] = v
                    pbar.update(1)
        cap_series = pd.Series(cap_data, name='ì‹œê°€ì´ì•¡')
        ok = cap_series.notna().sum()
        print(f"    âœ… ê°œë³„ ìˆ˜ì§‘: {ok}/{len(df)}ê°œ")

    # â”€â”€ ì‹œê°€ì´ì•¡ ì ìš© â”€â”€
    if not cap_series.empty and cap_series.notna().any():
        df['_ì‹œê°€ì´ì•¡_raw'] = cap_series
        valid = df['_ì‹œê°€ì´ì•¡_raw'].notna() & (df['_ì‹œê°€ì´ì•¡_raw'] >= min_cap * 1e8)
        df = df[valid].copy()
        df['ì‹œê°€ì´ì•¡(ì–µì›)'] = (df['_ì‹œê°€ì´ì•¡_raw'] / 1e8).round(0).astype(int)
        df = df.drop(columns=['_ì‹œê°€ì´ì•¡_raw'], errors='ignore')

        if not nav_series.empty and nav_series.notna().any():
            df['NAV(ì–µì›)'] = (nav_series.reindex(df.index) / 1e8).round(2)

        print(f"  â†’ ì‹œê°€ì´ì•¡ ë²”ìœ„: {df['ì‹œê°€ì´ì•¡(ì–µì›)'].min():,} ~ {df['ì‹œê°€ì´ì•¡(ì–µì›)'].max():,}ì–µì›")

        # ìºì‹œ ì €ì¥
        cache_df = df[['ì‹œê°€ì´ì•¡(ì–µì›)']].copy()
        if 'NAV(ì–µì›)' in df.columns:
            cache_df['NAV(ì–µì›)'] = df['NAV(ì–µì›)']
        _save_cache(cache_name, cache_df)
    else:
        print(f"  âš ï¸ ì‹œê°€ì´ì•¡ ìˆ˜ì§‘ ì‹¤íŒ¨ â€” í•„í„° ê±´ë„ˆëœ€")

    print(f"  â†’ {before}ê°œ â†’ {len(df)}ê°œ (ì‹œì´ {min_cap}ì–µ+ í•„í„°)")

    # ê¸°íƒ€ ì¹´í…Œê³ ë¦¬
    etc = df[df['ëŒ€ì¹´í…Œê³ ë¦¬'] == 'ê¸°íƒ€']
    if len(etc) > 0:
        print(f"\n  âš ï¸ [ê¸°íƒ€: {len(etc)}ê°œ]")
        for idx, row in etc.iterrows():
            print(f"    - {idx} {row['ETFëª…']}")

    print(f"  â±ï¸ Step 3: {time.time()-t0:.1f}ì´ˆ")
    return df


# ============================================================================
# Step 4: ìµœì¢… ë¦¬ìŠ¤íŠ¸ â†’ ê°€ê²© / ìƒì¥ì¼ / PDF ìˆ˜ì§‘ (ë¬´ê±°ìš´ ì‘ì—…)
# ============================================================================
def step4_collect_all_data(df, base_date):
    print("\n" + "="*60)
    print(f" Step 4: ê°€ê²© / ìƒì¥ì¼ / êµ¬ì„±ì¢…ëª© ìˆ˜ì§‘ ({len(df)}ê°œ ETF)")
    print("="*60)

    t0_total = time.time()
    tickers = df.index.tolist()

    # 4-A: ê°€ê²© + KOSPI
    t0 = time.time()
    df, df_close, kospi_close = _collect_prices(df, tickers, base_date)
    print(f"  â±ï¸ Step 4-A (ê°€ê²©): {time.time()-t0:.1f}ì´ˆ")

    # 4-B: ì„¤ì •ì¼
    t0 = time.time()
    df = _collect_listing_dates(df, tickers, base_date)
    print(f"  â±ï¸ Step 4-B (ì„¤ì •ì¼): {time.time()-t0:.1f}ì´ˆ")

    # 4-C: PDF â†’ ë³„ë„ df_pdf
    t0 = time.time()
    df_pdf = _collect_pdf_holdings(df, tickers, base_date)
    print(f"  â±ï¸ Step 4-C (PDF): {time.time()-t0:.1f}ì´ˆ")

    # 4-D: ìˆ˜ìµë¥  / BM / ìˆœìœ„
    t0 = time.time()
    df = _calc_returns(df, df_close, kospi_close, base_date)
    print(f"  â±ï¸ Step 4-D (ìˆ˜ìµë¥ ): {time.time()-t0:.1f}ì´ˆ")

    print(f"  â±ï¸ Step 4 ì „ì²´: {time.time()-t0_total:.1f}ì´ˆ")
    return df, df_close, df_pdf


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 4-A: ê°€ê²©
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _collect_prices(df, tickers, base_date):
    print("\n  â”€â”€ 4-A: ê°€ê²© ë°ì´í„° â”€â”€")

    base_dt = datetime.strptime(base_date, "%Y%m%d")
    start_date = (base_dt - timedelta(days=Config.PRICE_HISTORY_DAYS)).strftime("%Y%m%d")
    ytd_start = base_dt.replace(month=1, day=1).strftime("%Y%m%d")
    if ytd_start < start_date: start_date = ytd_start

    print(f"  â†’ ê¸°ê°„: {start_date} ~ {base_date}")

    # KOSPI
    print("  â†’ KOSPI ìˆ˜ì§‘...")
    try:
        kdf = stock.get_index_ohlcv_by_date(start_date, base_date, "1001")
        kospi = kdf['ì¢…ê°€'] if 'ì¢…ê°€' in kdf.columns else kdf.iloc[:, 3]
        kospi = kospi.sort_index()
        print(f"  â†’ KOSPI: {len(kospi)}ì¼")
    except Exception as e:
        print(f"  âš ï¸  KOSPI ì‹¤íŒ¨: {e}")
        kospi = pd.Series(dtype=float)

    # ìºì‹œ
    cache_file = os.path.join(Config.CACHE_DIR, f"price_v5_{base_date}.pkl")
    if Config.USE_CACHE and os.path.exists(cache_file):
        try:
            with open(cache_file, 'rb') as f:
                df_close = pickle.load(f)['close']
            common = [t for t in tickers if t in df_close.columns]
            if len(common) / max(len(tickers), 1) > 0.9:
                print(f"  â†’ ğŸ’¾ ìºì‹œ: {len(common)}ê°œ ETF")
                return df, df_close[common], kospi
        except Exception: pass

    # ë°©ë²• A: ë‚ ì§œ ì¼ê´„
    print("  â†’ ë‚ ì§œ ê¸°ì¤€ ì¼ê´„ ìˆ˜ì§‘...")
    df_close = _fetch_prices_bulk(tickers, start_date, base_date)

    # ë°©ë²• B: fallback
    if df_close.empty or df_close.shape[1] < len(tickers) * 0.5:
        print("  â†’ ê°œë³„ í‹°ì»¤ ìˆ˜ì§‘ ì „í™˜...")
        df_close = _fetch_prices_by_ticker(tickers, start_date, base_date)

    if not df_close.empty:
        print(f"  â†’ ê°€ê²©: {df_close.shape[0]}ì¼ Ã— {df_close.shape[1]}ê°œ ETF")

    if Config.USE_CACHE and not df_close.empty:
        os.makedirs(Config.CACHE_DIR, exist_ok=True)
        with open(cache_file, 'wb') as f:
            pickle.dump({'close': df_close}, f)

    return df, df_close, kospi


def _fetch_prices_bulk(tickers, start_date, base_date):
    try:
        sample = stock.get_etf_ohlcv_by_date(start_date, base_date, "069500")
        dates = [d.strftime("%Y%m%d") for d in sample.index]
    except Exception: return pd.DataFrame()

    print(f"  â†’ ì˜ì—…ì¼: {len(dates)}ì¼ / ìŠ¤ë ˆë“œ: {Config.MAX_WORKERS}")

    def fetch(d):
        try:
            r = stock.get_etf_price_by_ticker(d)
            time.sleep(Config.API_DELAY)
            if not r.empty and 'ì¢…ê°€' in r.columns: return d, r['ì¢…ê°€']
            elif not r.empty: return d, r.iloc[:, 0]
        except Exception: pass
        return d, None

    daily = {}
    t0 = time.time()
    with ThreadPoolExecutor(max_workers=Config.MAX_WORKERS) as exe:
        futs = {exe.submit(fetch, d): d for d in dates}
        with tqdm(total=len(dates), desc="  ë‚ ì§œë³„ ê°€ê²©") as pbar:
            for f in as_completed(futs):
                d, p = f.result()
                if p is not None: daily[d] = p
                pbar.update(1)
    print(f"  â±ï¸ {time.time()-t0:.1f}ì´ˆ")
    if not daily: return pd.DataFrame()

    out = pd.DataFrame(daily).T
    out.index = pd.to_datetime(out.index, format="%Y%m%d")
    out = out.sort_index()
    common = [t for t in tickers if t in out.columns]
    return out[common].apply(pd.to_numeric, errors='coerce')


def _fetch_prices_by_ticker(tickers, start_date, base_date):
    def fetch(t):
        try:
            o = stock.get_etf_ohlcv_by_date(start_date, base_date, t)
            time.sleep(Config.API_DELAY)
            if not o.empty and 'ì¢…ê°€' in o.columns: return t, o['ì¢…ê°€']
            elif not o.empty: return t, o.iloc[:, 3]
        except Exception: pass
        return t, None

    d = {}
    with ThreadPoolExecutor(max_workers=Config.MAX_WORKERS) as exe:
        futs = {exe.submit(fetch, t): t for t in tickers}
        with tqdm(total=len(tickers), desc="  í‹°ì»¤ë³„ ê°€ê²©") as pbar:
            for f in as_completed(futs):
                t, s = f.result()
                if s is not None: d[t] = s
                pbar.update(1)
    if not d: return pd.DataFrame()
    return pd.DataFrame(d).sort_index().apply(pd.to_numeric, errors='coerce')


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 4-B: ì„¤ì •ì¼ (1ì°¨ ë„¤ì´ë²„ â†’ 2ì°¨ pykrx)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _collect_listing_dates(df, tickers, base_date):
    print("\n  â”€â”€ 4-B: ì„¤ì •ì¼ ìˆ˜ì§‘ â”€â”€")

    cache_file = os.path.join(Config.CACHE_DIR, "listing_dates_v3.pkl")
    cached = {}
    if Config.USE_CACHE and os.path.exists(cache_file):
        try:
            with open(cache_file, 'rb') as f: cached = pickle.load(f)
        except Exception: cached = {}

    to_fetch = [t for t in tickers if t not in cached]
    print(f"  â†’ ì‹ ê·œ: {len(to_fetch)}ê°œ / ìºì‹œ: {len(tickers)-len(to_fetch)}ê°œ")

    if to_fetch:
        # 1ì°¨: ë„¤ì´ë²„
        print("  â†’ [1ì°¨] ë„¤ì´ë²„ ê¸ˆìœµ...")
        naver = _naver_listing_dates(to_fetch)
        cached.update(naver)

        # 2ì°¨: pykrx fallback
        missing = [t for t in to_fetch if not cached.get(t)]
        if missing:
            print(f"  â†’ [2ì°¨] pykrx fallback: {len(missing)}ê°œ...")
            pykrx = _pykrx_listing_dates(missing, base_date)
            cached.update(pykrx)

        if Config.USE_CACHE:
            os.makedirs(Config.CACHE_DIR, exist_ok=True)
            with open(cache_file, 'wb') as f: pickle.dump(cached, f)

    ok = sum(1 for t in tickers if cached.get(t))
    print(f"  â†’ ì„¤ì •ì¼ ì™„ë£Œ: {ok}/{len(tickers)}ê°œ")
    df['ì„¤ì •ì¼'] = df.index.map(lambda t: cached.get(t, ''))
    return df


def _naver_listing_dates(tickers):
    results = {}
    def fetch(ticker):
        try:
            url = f"https://finance.naver.com/item/main.naver?code={ticker}"
            req = Request(url, headers={'User-Agent': 'Mozilla/5.0'})
            html = urlopen(req, timeout=5).read().decode('euc-kr', errors='ignore')
            m = re.search(r'ì„¤ì •ì¼.*?(\d{4}\.\d{2}\.\d{2})', html, re.DOTALL)
            if not m: m = re.search(r'ìƒì¥ì¼.*?(\d{4}\.\d{2}\.\d{2})', html, re.DOTALL)
            if m: return ticker, m.group(1).replace('.', '-')
        except Exception: pass
        return ticker, ''

    with ThreadPoolExecutor(max_workers=Config.MAX_WORKERS) as exe:
        futs = {exe.submit(fetch, t): t for t in tickers}
        with tqdm(total=len(tickers), desc="  ë„¤ì´ë²„ ì„¤ì •ì¼") as pbar:
            for f in as_completed(futs):
                t, d = f.result()
                if d: results[t] = d
                pbar.update(1)
    print(f"  â†’ ë„¤ì´ë²„ ì„±ê³µ: {len(results)}/{len(tickers)}")
    return results


def _pykrx_listing_dates(tickers, base_date):
    results = {}
    def fetch(ticker):
        try:
            o = stock.get_etf_ohlcv_by_date("20020101", base_date, ticker)
            time.sleep(Config.API_DELAY)
            if not o.empty: return ticker, o.index[0].strftime("%Y-%m-%d")
        except Exception: pass
        return ticker, ''

    with ThreadPoolExecutor(max_workers=8) as exe:
        futs = {exe.submit(fetch, t): t for t in tickers}
        with tqdm(total=len(tickers), desc="  pykrx ì„¤ì •ì¼") as pbar:
            for f in as_completed(futs):
                t, d = f.result()
                if d: results[t] = d
                pbar.update(1)
    return results


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 4-C: PDF êµ¬ì„±ì¢…ëª© â†’ í”¼ë²— ë§¤íŠ¸ë¦­ìŠ¤
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _collect_pdf_holdings(df, tickers, base_date):
    """êµ¬ì„±ì¢…ëª© ìˆ˜ì§‘ â†’ í”¼ë²— ë§¤íŠ¸ë¦­ìŠ¤ df_pdf ë°˜í™˜
       í–‰: ETF í‹°ì»¤, ì—´: ì¢…ëª©ëª…(ã„±ã„´ã„·ìˆœ), ê°’: ë³´ìœ ë¹„ì¤‘(%)
    """
    print(f"\n  â”€â”€ 4-C: êµ¬ì„±ì¢…ëª© Top {Config.TOP_N_HOLDINGS} ìˆ˜ì§‘ â”€â”€")

    cache_file = os.path.join(Config.CACHE_DIR, f"holdings_v3_{base_date}.pkl")
    cached = {}
    if Config.USE_CACHE and os.path.exists(cache_file):
        try:
            with open(cache_file, 'rb') as f: cached = pickle.load(f)
        except Exception: cached = {}

    to_fetch = [t for t in tickers if t not in cached]
    print(f"  â†’ ì‹ ê·œ: {len(to_fetch)}ê°œ / ìºì‹œ: {len(tickers)-len(to_fetch)}ê°œ")

    if to_fetch:
        print("  â†’ pykrx PDF ì¡°íšŒ...")
        pykrx_results = _pykrx_holdings(to_fetch, base_date)
        cached.update(pykrx_results)

        missing_count = sum(1 for t in to_fetch if t not in cached or len(cached.get(t, [])) == 0)
        if missing_count:
            print(f"  â†’ ë¹„ì¤‘ ì—†ëŠ” ETF(í•´ì™¸ ë“±): {missing_count}ê°œ â†’ ë¹ˆì¹¸ ì²˜ë¦¬")

        if Config.USE_CACHE:
            os.makedirs(Config.CACHE_DIR, exist_ok=True)
            with open(cache_file, 'wb') as f: pickle.dump(cached, f)

    ok = sum(1 for t in tickers if cached.get(t) and len(cached[t]) > 0)
    print(f"  â†’ êµ¬ì„±ì¢…ëª© ì™„ë£Œ: {ok}/{len(tickers)}ê°œ")

    # â”€â”€ í”¼ë²— ë§¤íŠ¸ë¦­ìŠ¤ ìƒì„± â”€â”€
    # cached: {ticker: [(ì¢…ëª©ëª…, ë¹„ì¤‘%), ...]}
    # â†’ í–‰=ETF, ì—´=ì¢…ëª©ëª…(ã„±ã„´ã„·), ê°’=ë¹„ì¤‘
    all_stocks = set()
    for t in tickers:
        for name, w in cached.get(t, []):
            all_stocks.add(name)

    # ã„±ã„´ã„· ì •ë ¬
    sorted_stocks = sorted(all_stocks, key=lambda x: x)
    print(f"  â†’ ì „ì²´ ê³ ìœ  ì¢…ëª© ìˆ˜: {len(sorted_stocks)}ê°œ")

    # ETFëª… ì»¬ëŸ¼ + ì¢…ëª©ë³„ ë¹„ì¤‘ (ë¹ˆì¹¸ì€ NaN â†’ ì—‘ì…€ì—ì„œ ì •ë ¬ ê°€ëŠ¥)
    pdf_data = {}
    for ticker in tickers:
        row = {'ETFëª…': df.at[ticker, 'ETFëª…'] if ticker in df.index else ''}
        holdings_dict = {name: w for name, w in cached.get(ticker, [])}
        for s in sorted_stocks:
            row[s] = holdings_dict.get(s, np.nan)
        pdf_data[ticker] = row

    df_pdf = pd.DataFrame.from_dict(pdf_data, orient='index')
    df_pdf.index.name = 'í‹°ì»¤'

    # ë¹„ì¤‘ì´ ìˆëŠ” ì…€ ìˆ˜ í†µê³„
    stock_cols = [c for c in df_pdf.columns if c != 'ETFëª…']
    filled = df_pdf[stock_cols].notna().sum().sum()
    print(f"  â†’ ë§¤íŠ¸ë¦­ìŠ¤: {len(df_pdf)} ETF Ã— {len(sorted_stocks)} ì¢…ëª© ({filled:,.0f}ê°œ ì…€ ì±„ì›€)")

    return df_pdf


def _pykrx_holdings(tickers, base_date):
    """pykrx PDF: [(ì¢…ëª©ëª…, ë¹„ì¤‘%), ...] íŠœí”Œ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜"""
    results = {}

    # ì¢…ëª©ì½”ë“œ â†’ ì¢…ëª©ëª… ìºì‹œ
    stock_name_cache = {}
    def get_stock_name(code):
        if code in stock_name_cache:
            return stock_name_cache[code]
        try:
            name = stock.get_market_ticker_name(code)
            if name:
                stock_name_cache[code] = name
                return name
        except Exception:
            pass
        return code

    def fetch(ticker):
        try:
            pdf = stock.get_etf_portfolio_deposit_file(ticker, base_date)
            time.sleep(Config.API_DELAY)
            if pdf is None or pdf.empty:
                return ticker, []

            # ë¹„ì¤‘ ì»¬ëŸ¼ ì°¾ê¸°
            weight_col = None
            for c in pdf.columns:
                if 'ë¹„ì¤‘' in str(c) or 'êµ¬ì„±ë¹„' in str(c) or 'weight' in str(c).lower():
                    weight_col = c; break

            # ë¹„ì¤‘ ì»¬ëŸ¼ ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ (í•´ì™¸ ETF ë“±)
            if not weight_col:
                return ticker, []

            # ETF í‹°ì»¤ ëª©ë¡ (ETF-in-ETF ì œì™¸ìš©)
            try:
                etf_tickers_set = set(stock.get_etf_ticker_list(base_date))
            except Exception:
                etf_tickers_set = set()

            items = []
            pdf_sorted = pdf.sort_values(weight_col, ascending=False)
            for idx, row in pdf_sorted.head(Config.TOP_N_HOLDINGS + 5).iterrows():
                if len(items) >= Config.TOP_N_HOLDINGS:
                    break
                w = row[weight_col]
                if not pd.notna(w) or w <= 0:
                    continue
                code = str(idx)

                # 6ìë¦¬ ìˆ«ì ì½”ë“œì¸ ê²½ìš°
                if code.isdigit() and len(code) == 6:
                    # ETF-in-ETF â†’ ì œì™¸
                    if code in etf_tickers_set:
                        continue
                    name = get_stock_name(code)
                    # ì¢…ëª©ëª… ë³€í™˜ ì‹¤íŒ¨(ì—¬ì „íˆ ì½”ë“œ) â†’ ì œì™¸
                    if name == code:
                        continue
                else:
                    # í•œê¸€ì´ ì•„ë‹Œ ì•ŒíŒŒë²³/ìˆ«ì ì½”ë“œ â†’ ì œì™¸ (í•´ì™¸ì¢…ëª© ë“±)
                    if not re.search(r'[ê°€-í£]', code):
                        continue
                    name = code

                name = name[:20]
                items.append((name, round(float(w), 2)))

            return ticker, items
        except Exception:
            pass
        return ticker, []

    with ThreadPoolExecutor(max_workers=Config.MAX_WORKERS) as exe:
        futs = {exe.submit(fetch, t): t for t in tickers}
        with tqdm(total=len(tickers), desc="  pykrx PDF") as pbar:
            for f in as_completed(futs):
                t, items = f.result()
                if items: results[t] = items
                pbar.update(1)

    print(f"  â†’ pykrx ì„±ê³µ: {len(results)}/{len(tickers)}")
    return results


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 4-D: ìˆ˜ìµë¥  / BM / ìˆœìœ„
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _calc_returns(df, df_close, kospi_close, base_date):
    if df_close.empty or len(df_close) < 2:
        return df

    base_dt = datetime.strptime(base_date, "%Y%m%d")
    ytd_dt = base_dt.replace(month=1, day=1)
    ytd_mask = df_close.index >= pd.Timestamp(ytd_dt)
    ytd_loc = df_close.index.get_loc(df_close.index[ytd_mask][0]) if ytd_mask.any() else 0

    n = len(df_close)
    periods = {'1M': min(21, n-1), '3M': min(63, n-1),
               '6M': min(126, n-1), '1Y': n-1}

    for label, lb in periods.items():
        df[f'ìˆ˜ìµë¥ _{label}(%)'] = ((df_close.iloc[-1] / df_close.iloc[-1-lb] - 1) * 100).round(2)
    df['ìˆ˜ìµë¥ _YTD(%)'] = ((df_close.iloc[-1] / df_close.iloc[ytd_loc] - 1) * 100).round(2)

    dr = df_close.pct_change().dropna()
    if len(dr) > 20:
        df['ì—°ê°„ë³€ë™ì„±(%)'] = (dr.std() * np.sqrt(252) * 100).round(2)

    # KOSPI BM
    if not kospi_close.empty and len(kospi_close) > 1:
        kc = kospi_close.sort_index()
        kn = len(kc)
        bm = {}
        for label, lb in periods.items():
            klb = min(lb, kn-1)
            bm[label] = round((kc.iloc[-1] / kc.iloc[-1-klb] - 1) * 100, 2)

        k_ytd = kc.index >= pd.Timestamp(ytd_dt)
        bm['YTD'] = round((kc.iloc[-1] / kc[k_ytd].iloc[0] - 1) * 100, 2) if k_ytd.any() else 0.0

        print(f"\n  ğŸ“ˆ KOSPI BM (ê²€ì¦):")
        print(f"    ìµœì¢…: {kc.iloc[-1]:,.0f} ({kc.index[-1].strftime('%Y-%m-%d')})")
        k1m = -1 - min(21, kn-1)
        print(f"    1M ê¸°ì¤€: {kc.iloc[k1m]:,.0f} ({kc.index[k1m].strftime('%Y-%m-%d')})")
        for l, r in bm.items(): print(f"    {l}: {r:+.2f}%")

        for label in ['1M', '3M', '6M', '1Y']:
            df[f'BM_{label}(%)'] = (df[f'ìˆ˜ìµë¥ _{label}(%)'] - bm[label]).round(2)
        df['BM_YTD(%)'] = (df['ìˆ˜ìµë¥ _YTD(%)'] - bm['YTD']).round(2)

    # ìˆœìœ„
    if 'BM_YTD(%)' in df.columns:
        df['ìˆœìœ„(YTD_BM+)'] = df['BM_YTD(%)'].rank(ascending=False, method='min', na_option='bottom').astype(int)

    return df


# ============================================================================
# ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ (ë³€ê²½ ì—†ìŒ)
# ============================================================================
def _classify(df):
    def classify(name):
        n = str(name); u = n.upper()

        # ì›ìì¬
        if any(kw in u for kw in ['ê¸ˆí˜„ë¬¼','ê¸ˆì„ ë¬¼','ê³¨ë“œ','GOLD','êµ­ì œê¸ˆ','ê¸ˆì•¡í‹°ë¸Œ','ê¸ˆETF']): return 'ì›ìì¬','ê¸ˆ',''
        if any(kw in u for kw in ['ì€í˜„ë¬¼','ì€ì„ ë¬¼','ì‹¤ë²„','SILVER']): return 'ì›ìì¬','ì€',''
        if any(kw in u for kw in ['ì›ìœ ','WTI','ë¸Œë ŒíŠ¸','BRENT','ì˜¤ì¼']): return 'ì›ìì¬','ì›ìœ ',''
        if any(kw in u for kw in ['ì²œì—°ê°€ìŠ¤']): return 'ì›ìì¬','ì²œì—°ê°€ìŠ¤',''
        if any(kw in u for kw in ['êµ¬ë¦¬','íŒ”ë¼ë“','ë°±ê¸ˆ','í”Œë˜í‹°ë„˜','ë¹„ì² ']): return 'ì›ìì¬','ë¹„ì² ê¸ˆì†',''
        if any(kw in u for kw in ['ê³¡ë¬¼','ë†ì‚°ë¬¼','ì˜¥ìˆ˜ìˆ˜','ëŒ€ë‘','ë°€']): return 'ì›ìì¬','ë†ì‚°ë¬¼',''
        if any(kw in u for kw in ['ì›ìì¬','ì»¤ë¨¸ë””í‹°','COMMODITY']): return 'ì›ìì¬','ì›ìì¬(ì¢…í•©)',''

        # í†µí™”/í™˜ìœ¨
        if any(kw in u for kw in ['ë‹¬ëŸ¬','USD','ë‹¬ëŸ¬ì„ ë¬¼','ë¯¸êµ­ë‹¬ëŸ¬']): return 'í†µí™”/í™˜ìœ¨','ë‹¬ëŸ¬',''
        if any(kw in u for kw in ['ì—”í™”','ì—”ì„ ë¬¼','JPY']): return 'í†µí™”/í™˜ìœ¨','ì—”í™”',''
        if any(kw in u for kw in ['ìœ ë¡œ','EUR']): return 'í†µí™”/í™˜ìœ¨','ìœ ë¡œ',''
        if any(kw in u for kw in ['ìœ„ì•ˆ','CNY','CNH']): return 'í†µí™”/í™˜ìœ¨','ìœ„ì•ˆ',''
        if any(kw in u for kw in ['í™˜í—¤ì§€','í†µí™”','ì™¸í™˜','FX']): return 'í†µí™”/í™˜ìœ¨','í†µí™”(ê¸°íƒ€)',''

        # ë¦¬ì¸ /ë¶€ë™ì‚°
        if any(kw in u for kw in ['ë¦¬ì¸ ','REITS','REIT','ë¶€ë™ì‚°']): return 'ë¦¬ì¸ /ë¶€ë™ì‚°','ë¦¬ì¸ ',''

        # ê·¸ë£¹ì£¼
        if 'ê·¸ë£¹' in n:
            grp = n.split('ê·¸ë£¹')[0]
            for pfx in ['TIGER ','KODEX ','ACE ','KBSTAR ','SOL ','HANARO ','ARIRANG ','KOSEF ','PLUS ']:
                grp = grp.replace(pfx.strip(),'').strip()
            return 'ê·¸ë£¹ì£¼', grp+'ê·¸ë£¹', ''

        # í•´ì™¸ì£¼ì‹
        if any(kw in u for kw in ['ë¯¸êµ­','ë‚˜ìŠ¤ë‹¥','NASDAQ','S&P','S&P500','ë‹¤ìš°','í•„ë¼ë¸í”¼ì•„','FANG','NYSE','ë¯¸êµ­ë¹…í…Œí¬','ë¯¸êµ­í…Œí¬']):
            return 'í•´ì™¸ì£¼ì‹','ë¯¸êµ­',_sub(u,'ë¯¸êµ­')
        if any(kw in u for kw in ['ì¼ë³¸','ë‹ˆì¼€ì´','NIKKEI','TOPIX','ë„ì¿„']): return 'í•´ì™¸ì£¼ì‹','ì¼ë³¸',_sub(u,'ì¼ë³¸')
        if any(kw in u for kw in ['ì¤‘êµ­','ì°¨ì´ë‚˜','CSI','í•­ì…','HANG SENG','ì‹¬ì²œ','ìƒí•´','HSCEI','í™ì½©','CHINEXT','ë³¸í† ','CHINA']):
            return 'í•´ì™¸ì£¼ì‹','ì¤‘êµ­',_sub(u,'ì¤‘êµ­')
        if any(kw in u for kw in ['ì¸ë„','ë‹ˆí”„í‹°','NIFTY','INDIA']): return 'í•´ì™¸ì£¼ì‹','ì¸ë„',_sub(u,'ì¸ë„')
        if any(kw in u for kw in ['ë² íŠ¸ë‚¨','VN30','VIETNAM']): return 'í•´ì™¸ì£¼ì‹','ë² íŠ¸ë‚¨',_sub(u,'ë² íŠ¸ë‚¨')
        if any(kw in u for kw in ['ëŒ€ë§Œ','TAIWAN']): return 'í•´ì™¸ì£¼ì‹','ëŒ€ë§Œ',''
        if any(kw in u for kw in ['ìœ ëŸ½','EURO STOXX','ìœ ë¡œìŠ¤íƒìŠ¤','STOXX']): return 'í•´ì™¸ì£¼ì‹','ìœ ëŸ½',_sub(u,'ìœ ëŸ½')
        if any(kw in u for kw in ['ì¸ë„ë„¤ì‹œì•„']): return 'í•´ì™¸ì£¼ì‹','ì¸ë„ë„¤ì‹œì•„',''
        if any(kw in u for kw in ['ë¸Œë¼ì§ˆ']): return 'í•´ì™¸ì£¼ì‹','ë¸Œë¼ì§ˆ',''
        if any(kw in u for kw in ['ë©•ì‹œì½”']): return 'í•´ì™¸ì£¼ì‹','ë©•ì‹œì½”',''
        if any(kw in u for kw in ['ì‚¬ìš°ë””']): return 'í•´ì™¸ì£¼ì‹','ì‚¬ìš°ë””',''
        if any(kw in u for kw in ['ì„ ì§„êµ­','MSCI WORLD','ACWI','ê¸€ë¡œë²Œ']): return 'í•´ì™¸ì£¼ì‹','ê¸€ë¡œë²Œ/ì„ ì§„êµ­',''
        if any(kw in u for kw in ['ì‹ í¥êµ­','EM','EMERGING']): return 'í•´ì™¸ì£¼ì‹','ì‹ í¥êµ­',''

        # ì„¹í„°/í…Œë§ˆ
        if any(kw in u for kw in ['ë°˜ë„ì²´','íŒ¹ë¦¬ìŠ¤']): return 'ì„¹í„°/í…Œë§ˆ','ë°˜ë„ì²´',''
        if any(kw in u for kw in ['2ì°¨ì „ì§€','ë°°í„°ë¦¬','ë¦¬íŠ¬','ì–‘ê·¹ì¬','ìŒê·¹ì¬']): return 'ì„¹í„°/í…Œë§ˆ','2ì°¨ì „ì§€/ë°°í„°ë¦¬',''
        if any(kw in u for kw in ['AI','ì¸ê³µì§€ëŠ¥']): return 'ì„¹í„°/í…Œë§ˆ','AI',''
        if any(kw in u for kw in ['ì†Œí”„íŠ¸ì›¨ì–´','SW','í´ë¼ìš°ë“œ','SAAS']): return 'ì„¹í„°/í…Œë§ˆ','ì†Œí”„íŠ¸ì›¨ì–´/í´ë¼ìš°ë“œ',''
        if any(kw in u for kw in ['ê²Œì„','GAME']): return 'ì„¹í„°/í…Œë§ˆ','ê²Œì„',''
        if any(kw in u for kw in ['ì—”í„°','K-POP','KPOP','ì½˜í…ì¸ ']): return 'ì„¹í„°/í…Œë§ˆ','ì—”í„°/ì½˜í…ì¸ ',''
        if any(kw in u for kw in ['ë¯¸ë””ì–´','ë°©ì†¡','ê´‘ê³ ']): return 'ì„¹í„°/í…Œë§ˆ','ë¯¸ë””ì–´',''
        if any(kw in u for kw in ['ë°”ì´ì˜¤','í—¬ìŠ¤ì¼€ì–´','ì œì•½','ì˜ë£Œê¸°ê¸°','ì˜ë£Œ','í—¬ìŠ¤']): return 'ì„¹í„°/í…Œë§ˆ','ë°”ì´ì˜¤/í—¬ìŠ¤ì¼€ì–´',''
        if any(kw in u for kw in ['ìë™ì°¨','ì „ê¸°ì°¨','EV','ëª¨ë¹Œë¦¬í‹°','ììœ¨ì£¼í–‰']): return 'ì„¹í„°/í…Œë§ˆ','ìë™ì°¨/ëª¨ë¹Œë¦¬í‹°',''
        if any(kw in u for kw in ['ë¡œë´‡','ìë™í™”','ë¡œë³´í‹±ìŠ¤']): return 'ì„¹í„°/í…Œë§ˆ','ë¡œë´‡/ìë™í™”',''
        if any(kw in u for kw in ['ì€í–‰']): return 'ì„¹í„°/í…Œë§ˆ','ì€í–‰',''
        if any(kw in u for kw in ['ì¦ê¶Œ']): return 'ì„¹í„°/í…Œë§ˆ','ì¦ê¶Œ',''
        if any(kw in u for kw in ['ë³´í—˜']): return 'ì„¹í„°/í…Œë§ˆ','ë³´í—˜',''
        if any(kw in u for kw in ['ê¸ˆìœµ']): return 'ì„¹í„°/í…Œë§ˆ','ê¸ˆìœµ(ê¸°íƒ€)',''
        if any(kw in u for kw in ['ê±´ì„¤','ì¸í”„ë¼','ì‹œë©˜íŠ¸']): return 'ì„¹í„°/í…Œë§ˆ','ê±´ì„¤/ì¸í”„ë¼',''
        if any(kw in u for kw in ['ì¡°ì„ ']): return 'ì„¹í„°/í…Œë§ˆ','ì¡°ì„ ',''
        if any(kw in u for kw in ['í•´ìš´']): return 'ì„¹í„°/í…Œë§ˆ','í•´ìš´',''
        if any(kw in u for kw in ['ë°©ì‚°','ë°©ìœ„','ìš°ì£¼í•­ê³µ','í•­ê³µìš°ì£¼','ìš°ì£¼']): return 'ì„¹í„°/í…Œë§ˆ','ë°©ì‚°/ìš°ì£¼í•­ê³µ',''
        if any(kw in u for kw in ['í™”í•™','ì†Œì¬','ì‹ ì†Œì¬']): return 'ì„¹í„°/í…Œë§ˆ','í™”í•™/ì†Œì¬',''
        if any(kw in u for kw in ['ì² ê°•','ê¸ˆì†']): return 'ì„¹í„°/í…Œë§ˆ','ì² ê°•/ê¸ˆì†',''
        if any(kw in u for kw in ['ì—ë„ˆì§€','ì„ìœ ']): return 'ì„¹í„°/í…Œë§ˆ','ì—ë„ˆì§€',''
        if any(kw in u for kw in ['ìœ í‹¸ë¦¬í‹°','ì „ë ¥','ë°œì „']): return 'ì„¹í„°/í…Œë§ˆ','ìœ í‹¸ë¦¬í‹°/ì „ë ¥',''
        if any(kw in u for kw in ['í•„ìˆ˜ì†Œë¹„ì¬','ìŒì‹ë£Œ','ì‹í’ˆ','F&B']): return 'ì„¹í„°/í…Œë§ˆ','í•„ìˆ˜ì†Œë¹„ì¬/ì‹í’ˆ',''
        if any(kw in u for kw in ['ê²½ê¸°ì†Œë¹„ì¬','ëŸ­ì…”ë¦¬','í™”ì¥í’ˆ','ë·°í‹°']): return 'ì„¹í„°/í…Œë§ˆ','ê²½ê¸°ì†Œë¹„ì¬/ë·°í‹°',''
        if any(kw in u for kw in ['ì†Œë¹„ì¬']): return 'ì„¹í„°/í…Œë§ˆ','ì†Œë¹„ì¬(ê¸°íƒ€)',''
        if any(kw in u for kw in ['í†µì‹ ','5G','6G','í…”ë ˆì½¤']): return 'ì„¹í„°/í…Œë§ˆ','í†µì‹ ',''
        if any(kw in u for kw in ['ìš´ì†¡','ë¬¼ë¥˜','íƒë°°']): return 'ì„¹í„°/í…Œë§ˆ','ìš´ì†¡/ë¬¼ë¥˜',''
        if any(kw in u for kw in ['í•­ê³µ']) and 'ìš°ì£¼' not in u: return 'ì„¹í„°/í…Œë§ˆ','í•­ê³µ',''
        if any(kw in u for kw in ['ESG','íƒ„ì†Œ','ê·¸ë¦°','ì¹œí™˜ê²½']): return 'ì„¹í„°/í…Œë§ˆ','ESG/ì¹œí™˜ê²½',''
        if any(kw in u for kw in ['ìˆ˜ì†Œ','íƒœì–‘ê´‘','í’ë ¥','ì‹ ì¬ìƒ','ì¬ìƒì—ë„ˆì§€']): return 'ì„¹í„°/í…Œë§ˆ','ìˆ˜ì†Œ/ì‹ ì¬ìƒì—ë„ˆì§€',''
        if any(kw in u for kw in ['ì›ìë ¥','ì›ì „','ìš°ë¼ëŠ„','SMR']): return 'ì„¹í„°/í…Œë§ˆ','ì›ìë ¥',''
        if any(kw in u for kw in ['ì‚¬ì´ë²„ë³´ì•ˆ','ë³´ì•ˆ','ì‹œíë¦¬í‹°']): return 'ì„¹í„°/í…Œë§ˆ','ì‚¬ì´ë²„ë³´ì•ˆ',''
        if any(kw in u for kw in ['ë©”íƒ€ë²„ìŠ¤','XR','VR','AR']): return 'ì„¹í„°/í…Œë§ˆ','ë©”íƒ€ë²„ìŠ¤/XR',''
        if any(kw in u for kw in ['ë¸”ë¡ì²´ì¸','ë””ì§€í„¸ìì‚°','ë¹„íŠ¸ì½”ì¸','ê°€ìƒìì‚°','í¬ë¦½í† ']): return 'ì„¹í„°/í…Œë§ˆ','ë¸”ë¡ì²´ì¸/ê°€ìƒìì‚°',''
        if any(kw in u for kw in ['í”Œë«í¼','ì¸í„°ë„·','ì´ì»¤ë¨¸ìŠ¤','ì»¤ë¨¸ìŠ¤']): return 'ì„¹í„°/í…Œë§ˆ','í”Œë«í¼/ì¸í„°ë„·',''
        if any(kw in u for kw in ['IT','í…Œí¬','ê¸°ìˆ ','ICT']): return 'ì„¹í„°/í…Œë§ˆ','IT/í…Œí¬',''

        # ë°°ë‹¹/ì¸ì»´
        if any(kw in u for kw in ['ë°°ë‹¹','ê³ ë°°ë‹¹','ë°°ë‹¹ì„±ì¥','DIVIDEND','í”„ë¦¬ë¯¸ì—„','ì›”ë°°ë‹¹','ë¶„ë°°','ì¸ì»´']):
            return 'ë°°ë‹¹/ì¸ì»´','ë°°ë‹¹',''

        # ì‹œì¥ëŒ€í‘œ
        if re.search(r'200(?:TR)?$', n.strip().upper()) or 'KOSPI200' in u or 'KOSPI 200' in u: return 'ì‹œì¥ëŒ€í‘œ','KOSPI200',''
        if any(kw in u for kw in ['ì½”ìŠ¤ë‹¥','KOSDAQ']): return 'ì‹œì¥ëŒ€í‘œ','ì½”ìŠ¤ë‹¥',''
        if any(kw in u for kw in ['ì¤‘ì†Œí˜•','ì¤‘ì†Œ']): return 'ì‹œì¥ëŒ€í‘œ','ì¤‘ì†Œí˜•ì£¼',''
        if any(kw in u for kw in ['ì¤‘í˜•','ë¯¸ë“œìº¡','MID']): return 'ì‹œì¥ëŒ€í‘œ','ì¤‘ì†Œí˜•ì£¼',''
        if any(kw in u for kw in ['ì†Œí˜•','ìŠ¤ëª°ìº¡','SMALL']): return 'ì‹œì¥ëŒ€í‘œ','ì†Œí˜•ì£¼',''
        if any(kw in u for kw in ['ì½”ìŠ¤í”¼','KOSPI','KRX300','KRX 300','TOP10','TOP30','TOP 10','ëŒ€í˜•']):
            return 'ì‹œì¥ëŒ€í‘œ','ëŒ€í˜•ì£¼',''

        # ìŠ¤ë§ˆíŠ¸ë² íƒ€
        if any(kw in u for kw in ['ëª¨ë©˜í…€','MOMENTUM']): return 'ìŠ¤ë§ˆíŠ¸ë² íƒ€','ëª¨ë©˜í…€',''
        if any(kw in u for kw in ['ë°¸ë¥˜','ê°€ì¹˜','VALUE']): return 'ìŠ¤ë§ˆíŠ¸ë² íƒ€','ë°¸ë¥˜',''
        if any(kw in u for kw in ['í€„ë¦¬í‹°','QUALITY','ìš°ëŸ‰']): return 'ìŠ¤ë§ˆíŠ¸ë² íƒ€','í€„ë¦¬í‹°',''
        if any(kw in u for kw in ['ë¡œìš°ë³¼','ì €ë³€ë™','LOW VOL']): return 'ìŠ¤ë§ˆíŠ¸ë² íƒ€','ì €ë³€ë™ì„±',''
        if any(kw in u for kw in ['ë™ì¼ê°€ì¤‘','EQUAL']): return 'ìŠ¤ë§ˆíŠ¸ë² íƒ€','ë™ì¼ê°€ì¤‘',''
        if any(kw in u for kw in ['ì„±ì¥','GROWTH']): return 'ìŠ¤ë§ˆíŠ¸ë² íƒ€','ì„±ì¥',''
        if any(kw in u for kw in ['ë©€í‹°íŒ©í„°']): return 'ìŠ¤ë§ˆíŠ¸ë² íƒ€','ë©€í‹°íŒ©í„°',''

        return 'ê¸°íƒ€','ê¸°íƒ€',''

    results = df['ETFëª…'].apply(classify)
    df['ëŒ€ì¹´í…Œê³ ë¦¬'] = results.apply(lambda x: x[0])
    df['ì¤‘ì¹´í…Œê³ ë¦¬'] = results.apply(lambda x: x[1])
    df['ì†Œì¹´í…Œê³ ë¦¬'] = results.apply(lambda x: x[2])

    print("\n  [ëŒ€ì¹´í…Œê³ ë¦¬]")
    for c, n in df['ëŒ€ì¹´í…Œê³ ë¦¬'].value_counts().items(): print(f"    {c}: {n}ê°œ")
    print("\n  [ì¤‘ì¹´í…Œê³ ë¦¬ ìƒìœ„ 20]")
    for c, n in df['ì¤‘ì¹´í…Œê³ ë¦¬'].value_counts().head(20).items(): print(f"    {c}: {n}ê°œ")
    return df


def _sub(u, country):
    if country == 'ë¯¸êµ­':
        if any(kw in u for kw in ['S&P','S&P500']): return 'S&P500'
        if any(kw in u for kw in ['ë‚˜ìŠ¤ë‹¥','NASDAQ']): return 'ë‚˜ìŠ¤ë‹¥'
        if any(kw in u for kw in ['ë‹¤ìš°','DOW']): return 'ë‹¤ìš°'
        if any(kw in u for kw in ['ë°˜ë„ì²´','í•„ë¼ë¸í”¼ì•„']): return 'ë¯¸êµ­ë°˜ë„ì²´'
        if any(kw in u for kw in ['ë¹…í…Œí¬','í…Œí¬','FANG','TECH']): return 'ë¯¸êµ­í…Œí¬'
        if any(kw in u for kw in ['ë°°ë‹¹','DIVIDEND']): return 'ë¯¸êµ­ë°°ë‹¹'
        if any(kw in u for kw in ['í—¬ìŠ¤','ë°”ì´ì˜¤','ì œì•½']): return 'ë¯¸êµ­í—¬ìŠ¤ì¼€ì–´'
        if any(kw in u for kw in ['ê¸ˆìœµ','ì€í–‰']): return 'ë¯¸êµ­ê¸ˆìœµ'
        if any(kw in u for kw in ['ì„±ì¥','GROWTH']): return 'ë¯¸êµ­ì„±ì¥'
        if any(kw in u for kw in ['ê°€ì¹˜','VALUE']): return 'ë¯¸êµ­ê°€ì¹˜'
        if any(kw in u for kw in ['AI','ì¸ê³µì§€ëŠ¥']): return 'ë¯¸êµ­AI'
        if any(kw in u for kw in ['ë°©ì‚°','ìš°ì£¼í•­ê³µ','ë°©ìœ„']): return 'ë¯¸êµ­ë°©ì‚°'
        if any(kw in u for kw in ['ë¦¬ì¸ ','REITS']): return 'ë¯¸êµ­ë¦¬ì¸ '
        if any(kw in u for kw in ['ì›ìë ¥','ìš°ë¼ëŠ„','SMR']): return 'ë¯¸êµ­ì›ìë ¥'
        return 'ë¯¸êµ­(ê¸°íƒ€)'
    if country == 'ì¼ë³¸':
        if any(kw in u for kw in ['ë‹ˆì¼€ì´','NIKKEI']): return 'ë‹ˆì¼€ì´225'
        if 'TOPIX' in u: return 'TOPIX'
        if 'ë°˜ë„ì²´' in u: return 'ì¼ë³¸ë°˜ë„ì²´'
        return 'ì¼ë³¸(ê¸°íƒ€)'
    if country == 'ì¤‘êµ­':
        if any(kw in u for kw in ['CSI300','CSI 300']): return 'CSI300'
        if any(kw in u for kw in ['í•­ì…','HANG SENG','HSCEI']): return 'í•­ì…'
        if any(kw in u for kw in ['ì‹¬ì²œ','CHINEXT']): return 'ì‹¬ì²œ/ì°¨ì´ë„¥ìŠ¤íŠ¸'
        if 'CSI' in u: return 'CSI(ê¸°íƒ€)'
        return 'ì¤‘êµ­(ê¸°íƒ€)'
    if country == 'ì¸ë„':
        if any(kw in u for kw in ['ë‹ˆí”„í‹°','NIFTY']): return 'ë‹ˆí”„í‹°50'
        return 'ì¸ë„(ê¸°íƒ€)'
    if country == 'ë² íŠ¸ë‚¨':
        if 'VN30' in u: return 'VN30'
        return 'ë² íŠ¸ë‚¨(ê¸°íƒ€)'
    if country == 'ìœ ëŸ½':
        if any(kw in u for kw in ['STOXX','ìœ ë¡œìŠ¤íƒìŠ¤']): return 'EURO STOXX'
        return 'ìœ ëŸ½(ê¸°íƒ€)'
    return ''


# ============================================================================
# Step 5: ì €ì¥
# ============================================================================
def step5_save(df, df_close, df_pdf, base_date):
    print("\n" + "="*60)
    print(" Step 5: ìœ ë‹ˆë²„ìŠ¤ ì €ì¥")
    print("="*60)

    os.makedirs(Config.OUTPUT_DIR, exist_ok=True)
    if 'ì‹œê°€ì´ì•¡(ì–µì›)' in df.columns:
        df = df.sort_values('ì‹œê°€ì´ì•¡(ì–µì›)', ascending=False)

    print(f"\n  ğŸ“Š ìµœì¢…: {len(df)}ê°œ ETF")
    if 'ì‹œê°€ì´ì•¡(ì–µì›)' in df.columns:
        total = df['ì‹œê°€ì´ì•¡(ì–µì›)'].sum()
        print(f"  ğŸ’° ì‹œì´: {total:,.0f}ì–µì› ({total/10000:.1f}ì¡°ì›)")

    if 'ëŒ€ì¹´í…Œê³ ë¦¬' in df.columns:
        print("\n  [ëŒ€ì¹´í…Œê³ ë¦¬]")
        for cat in df['ëŒ€ì¹´í…Œê³ ë¦¬'].value_counts().index:
            sub = df[df['ëŒ€ì¹´í…Œê³ ë¦¬']==cat]
            cap = sub['ì‹œê°€ì´ì•¡(ì–µì›)'].sum() if 'ì‹œê°€ì´ì•¡(ì–µì›)' in sub.columns else 0
            print(f"    {cat}: {len(sub)}ê°œ ({cap:,.0f}ì–µ)")

    # â”€â”€ df_universe ì»¬ëŸ¼ ìˆœì„œ (Top ì—†ìŒ) â”€â”€
    cols = ['ETFëª…', 'ì‹œê°€ì´ì•¡(ì–µì›)', 'NAV(ì–µì›)', 'ì„¤ì •ì¼',
            'ëŒ€ì¹´í…Œê³ ë¦¬', 'ì¤‘ì¹´í…Œê³ ë¦¬', 'ì†Œì¹´í…Œê³ ë¦¬',
            'ìˆœìœ„(YTD_BM+)',
            'ìˆ˜ìµë¥ _1M(%)', 'ìˆ˜ìµë¥ _3M(%)', 'ìˆ˜ìµë¥ _6M(%)',
            'ìˆ˜ìµë¥ _1Y(%)', 'ìˆ˜ìµë¥ _YTD(%)',
            'BM_1M(%)', 'BM_3M(%)', 'BM_6M(%)',
            'BM_1Y(%)', 'BM_YTD(%)',
            'ì—°ê°„ë³€ë™ì„±(%)', 'ì¢…ê°€', 'ê±°ë˜ëŸ‰']

    cols = [c for c in cols if c in df.columns]
    df_export = df[cols].copy().fillna('')

    # â”€â”€ ì—‘ì…€: ìœ ë‹ˆë²„ìŠ¤ + PDF ì‹œíŠ¸ ë¶„ë¦¬ â”€â”€
    f_master = os.path.join(Config.OUTPUT_DIR, f"etf_universe_{base_date}.xlsx")
    with pd.ExcelWriter(f_master, engine='openpyxl') as writer:
        df_export.to_excel(writer, sheet_name='ìœ ë‹ˆë²„ìŠ¤')
        if df_pdf is not None and not df_pdf.empty:
            # df_pdfë„ ì‹œê°€ì´ì•¡ ìˆœìœ¼ë¡œ ì •ë ¬
            pdf_order = [t for t in df.index if t in df_pdf.index]
            df_pdf_sorted = df_pdf.loc[[t for t in pdf_order if t in df_pdf.index]]
            df_pdf_sorted.to_excel(writer, sheet_name='êµ¬ì„±ì¢…ëª©(PDF)')
    print(f"\n  ğŸ“ ìœ ë‹ˆë²„ìŠ¤ + PDF: {f_master}")

    # â”€â”€ ì¢…ê°€ CSV â”€â”€
    if df_close is not None and not df_close.empty:
        f_p = os.path.join(Config.OUTPUT_DIR, f"etf_prices_{base_date}.csv")
        df_close.to_csv(f_p, encoding='utf-8-sig')
        print(f"  ğŸ“ ì¢…ê°€: {f_p}")

    # â”€â”€ í‹°ì»¤ CSV â”€â”€
    f_t = os.path.join(Config.OUTPUT_DIR, f"etf_tickers_{base_date}.csv")
    tc = [c for c in ['ETFëª…','ì„¤ì •ì¼','ëŒ€ì¹´í…Œê³ ë¦¬','ì¤‘ì¹´í…Œê³ ë¦¬','ì†Œì¹´í…Œê³ ë¦¬'] if c in df.columns]
    df[tc].to_csv(f_t, encoding='utf-8-sig')
    print(f"  ğŸ“ í‹°ì»¤: {f_t}")

    # â”€â”€ Top 15 ì¶œë ¥ â”€â”€
    print(f"\n  {'â”€'*110}")
    print(f"  ğŸ“‹ Top 15 (ì‹œê°€ì´ì•¡)")
    print(f"  {'â”€'*110}")
    for i, (idx, r) in enumerate(df.head(15).iterrows(), 1):
        nm = str(r.get('ETFëª…',''))[:24]
        cap = f"{r['ì‹œê°€ì´ì•¡(ì–µì›)']:>7,}ì–µ" if r.get('ì‹œê°€ì´ì•¡(ì–µì›)','') != '' else ""
        c1 = str(r.get('ëŒ€ì¹´í…Œê³ ë¦¬',''))[:8]
        c2 = str(r.get('ì¤‘ì¹´í…Œê³ ë¦¬',''))[:10]
        ytd = f"{r['ìˆ˜ìµë¥ _YTD(%)']:+.2f}%" if r.get('ìˆ˜ìµë¥ _YTD(%)','') != '' else "N/A"
        bm = f"{r['BM_YTD(%)']:+.2f}%" if r.get('BM_YTD(%)','') != '' else "N/A"
        rnk = str(r.get('ìˆœìœ„(YTD_BM+)',''))
        # PDF top1 ê°€ì ¸ì˜¤ê¸° (í”¼ë²— ë§¤íŠ¸ë¦­ìŠ¤ì—ì„œ ìµœëŒ€ ë¹„ì¤‘ ì¢…ëª©)
        top1 = ''
        if df_pdf is not None and idx in df_pdf.index:
            row_pdf = df_pdf.loc[idx].drop('ETFëª…', errors='ignore')
            num_vals = pd.to_numeric(row_pdf, errors='coerce')
            if num_vals.notna().any():
                max_stock = num_vals.idxmax()
                max_w = num_vals.max()
                top1 = f"{max_stock}({max_w:.1f}%)"
        print(f"  {i:>3}. [{idx}] {nm:<26} {cap} {c1:<8} {c2:<12} YTD:{ytd:>8} BM:{bm:>8} #{rnk:<4} {top1}")

    return df_export


# ============================================================================
# ë©”ì¸
# ============================================================================
def build_universe():
    print("â•”" + "â•"*58 + "â•—")
    print("â•‘   í•œêµ­ ìƒì¥ ETF ìœ ë‹ˆë²„ìŠ¤ ë¹Œë” v5.3                       â•‘")
    print("â•š" + "â•"*58 + "â•")

    t_start = time.time()

    base_date = Config.BASE_DATE or find_latest_business_date()
    Config.BASE_DATE = base_date    # ì €ì¥
    print(f"\n  ğŸ“… ê¸°ì¤€ì¼: {base_date}")
    print(f"  ğŸ’° ìµœì†Œ ì‹œì´: {Config.MIN_MARKET_CAP_BILLIONS}ì–µì›")
    print(f"  âš¡ ìŠ¤ë ˆë“œ: {Config.MAX_WORKERS} / ìºì‹œ: {Config.USE_CACHE}")

    # Step 1: ê°€ë²¼ìš´ â€” í‹°ì»¤+ì´ë¦„ë§Œ
    df = step1_get_tickers_and_names(base_date)

    # Step 2: ê°€ë²¼ìš´ â€” ìœ í˜• í•„í„° (í‚¤ì›Œë“œ)
    df = step2_type_filter_and_classify(df)

    # Step 3: ì¤‘ê°„ â€” ì‹œì´ ë°ì´í„° â†’ í•„í„°
    df = step3_market_cap_filter(df, base_date, Config.MIN_MARKET_CAP_BILLIONS)

    # Step 4: ë¬´ê±°ìš´ â€” ìµœì¢… ë¦¬ìŠ¤íŠ¸ì—ë§Œ ê°€ê²©/ìƒì¥ì¼/PDF
    df, df_close, df_pdf = step4_collect_all_data(df, base_date)

    # Step 5: ì €ì¥
    step5_save(df, df_close, df_pdf, base_date)

    elapsed = time.time() - t_start
    print(f"\n{'='*60}")
    print(f" âœ… ì™„ë£Œ! ì´ ì†Œìš”ì‹œê°„: {elapsed:.0f}ì´ˆ ({elapsed/60:.1f}ë¶„)")
    print(f"{'='*60}")
    return df, df_close, df_pdf


if __name__ == "__main__":
    df_universe, df_prices, df_pdf = build_universe()
